{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_ib6JkOqkqj"
      },
      "outputs": [],
      "source": [
        "%pylab inline\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "np.set_printoptions(suppress=True)\n",
        "!python --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt92LC26qkq_"
      },
      "outputs": [],
      "source": [
        "# load the mnist dataset\n",
        "\n",
        "def fetch(url):\n",
        "  import requests, gzip, os, hashlib, numpy\n",
        "  fp = os.path.join(\"/tmp\", hashlib.md5(url.encode('utf-8')).hexdigest())\n",
        "  if os.path.isfile(fp):\n",
        "    with open(fp, \"rb\") as f:\n",
        "      dat = f.read()\n",
        "  else:\n",
        "    with open(fp, \"wb\") as f:\n",
        "      dat = requests.get(url).content\n",
        "      f.write(dat)\n",
        "  return numpy.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy()\n",
        "X_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[0x10:].reshape((-1, 28, 28))\n",
        "Y_train = fetch(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:]\n",
        "X_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[0x10:].reshape((-1, 28, 28))\n",
        "Y_test = fetch(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7ahsi2uqkrB"
      },
      "outputs": [],
      "source": [
        "# skip to the bottom for numpy only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvbHE2ShqkrC"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "class BobNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BobNet, self).__init__()\n",
        "    self.l1 = nn.Linear(784, 128, bias=False)\n",
        "    self.l2 = nn.Linear(128, 10, bias=False)\n",
        "    self.sm = nn.LogSoftmax(dim=1)\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.l1(x))\n",
        "    x = self.l2(x)\n",
        "    x = self.sm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtbzke8EqkrF"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "model = BobNet()\n",
        "print(\"model\",model)\n",
        "\n",
        "# protip: if you like accuracy like 96 not like 93, next time through the notebook, consider\n",
        "# CHAD MODE WEIGHT INIT WITH NUMPY\n",
        "# instead of virgin torch init mode\n",
        "# TODO: why is torch linear init bad?\n",
        "\"\"\"\n",
        "with torch.no_grad():\n",
        "  model.l1.weight.copy_(torch.tensor(layer_init(784, 128).T))\n",
        "  model.l2.weight.copy_(torch.tensor(layer_init(128, 10).T))\n",
        "\"\"\"\n",
        "\n",
        "loss_function = nn.NLLLoss(reduction='none')\n",
        "optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0)\n",
        "BS = 128\n",
        "losses, accuracies = [], []\n",
        "\n",
        "for t  in  range(1,1000):\n",
        "  samp = np.random.randint(0, X_train.shape[0], size=(BS))\n",
        "  X = torch.tensor(X_train[samp].reshape((-1, 28*28))).float()\n",
        "  Y = torch.tensor(Y_train[samp]).long()\n",
        "  model.zero_grad()\n",
        "  out = model(X)\n",
        "  cat = torch.argmax(out, dim=1)\n",
        "  accuracy = (cat == Y).float().mean()\n",
        "  loss = loss_function(out, Y)\n",
        "  loss = loss.mean()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  loss, accuracy = loss.item(), accuracy.item()\n",
        "  losses.append(loss)\n",
        "  accuracies.append(accuracy)\n",
        "  print(\"loss %.2f accuracy %.2f\" % (loss, accuracy))\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plot(losses)\n",
        "plot(accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu1GdenqqkrH"
      },
      "outputs": [],
      "source": [
        "# evaluation\n",
        "Y_test_preds = torch.argmax(model(torch.tensor(X_test.reshape((-1, 28*28))).float()), dim=1).numpy()\n",
        "(Y_test == Y_test_preds).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VIPcdZyGqkrQ"
      },
      "outputs": [],
      "source": [
        "# compute gradients in torch\n",
        "samp = [0,1,2,3]\n",
        "model.zero_grad()\n",
        "out = model(torch.tensor(X_test[samp].reshape((-1, 28*28))).float())\n",
        "out.retain_grad()\n",
        "loss = loss_function(out, torch.tensor(Y_test[samp]).long()).mean()\n",
        "loss.retain_grad()\n",
        "loss.backward()\n",
        "imshow(model.l1.weight.grad)\n",
        "figure()\n",
        "imshow(model.l2.weight.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSDopcOEqkrS"
      },
      "outputs": [],
      "source": [
        "# copy weights from pytorch\n",
        "l1 = model.l1.weight.detach().numpy().T\n",
        "l2 = model.l2.weight.detach().numpy().T\n",
        "\n",
        "# numpy forward pass\n",
        "def forward(x):\n",
        "  x = x.dot(l1)\n",
        "  x = np.maximum(x, 0)\n",
        "  x = x.dot(l2)  \n",
        "  return x\n",
        "\n",
        "def numpy_eval():\n",
        "  Y_test_preds_out = forward(X_test.reshape((-1, 28*28)))\n",
        "  Y_test_preds = np.argmax(Y_test_preds_out, axis=1)\n",
        "  return (Y_test == Y_test_preds).mean()\n",
        "\n",
        "numpy_eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kCxA3ehqkrU"
      },
      "outputs": [],
      "source": [
        "# numpy forward and backward pass\n",
        "\n",
        "def logsumexp(x):\n",
        "  #return np.log(np.exp(x).sum(axis=1))\n",
        "  # http://gregorygundersen.com/blog/2020/02/09/log-sum-exp/\n",
        "  c = x.max(axis=1)\n",
        "  return c + np.log(np.exp(x-c.reshape((-1, 1))).sum(axis=1))\n",
        "\n",
        "def forward_backward(x, y):\n",
        "  # training\n",
        "  out = np.zeros((len(y),10), np.float32)\n",
        "  out[range(out.shape[0]),y] = 1\n",
        "\n",
        "  # forward pass\n",
        "  x_l1 = x.dot(l1)\n",
        "  x_relu = np.maximum(x_l1, 0)\n",
        "  x_l2 = x_relu.dot(l2)\n",
        "  x_lsm = x_l2 - logsumexp(x_l2).reshape((-1, 1))\n",
        "  x_loss = (-out * x_lsm).mean(axis=1)\n",
        "\n",
        "  # training in numpy (super hard!)\n",
        "  # backward pass\n",
        "\n",
        "  # will involve x_lsm, x_l2, out, d_out and produce dx_sm\n",
        "  d_out = -out / len(y)\n",
        "\n",
        "  # derivative of logsoftmax\n",
        "  # https://github.com/torch/nn/blob/master/lib/THNN/generic/LogSoftMax.c\n",
        "  dx_lsm = d_out - np.exp(x_lsm)*d_out.sum(axis=1).reshape((-1, 1))\n",
        "\n",
        "  # derivative of l2\n",
        "  d_l2 = x_relu.T.dot(dx_lsm)\n",
        "  dx_relu = dx_lsm.dot(l2.T)\n",
        "\n",
        "  # derivative of relu\n",
        "  dx_l1 = (x_relu > 0).astype(np.float32) * dx_relu\n",
        "\n",
        "  # derivative of l1\n",
        "  d_l1 = x.T.dot(dx_l1)\n",
        "  \n",
        "  return x_loss, x_l2, d_l1, d_l2\n",
        "\n",
        "samp = [0,1,2,3]\n",
        "x_loss, x_l2, d_l1, d_l2 = forward_backward(X_test[samp].reshape((-1, 28*28)), Y_test[samp])\n",
        "\n",
        "imshow(d_l1.T)\n",
        "figure()\n",
        "imshow(d_l2.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-VFhp3SqkrW"
      },
      "outputs": [],
      "source": [
        "# numpy training\n",
        "def layer_init(m, h):\n",
        "  # gaussian is strong\n",
        "  #ret = np.random.randn(m,h)/np.sqrt(m*h)\n",
        "  # uniform is stronger\n",
        "  ret = np.random.uniform(-1., 1., size=(m,h))/np.sqrt(m*h)\n",
        "  return ret.astype(np.float32)\n",
        "\n",
        "# reinit\n",
        "np.random.seed(1337)\n",
        "l1 = layer_init(784, 128)\n",
        "l2 = layer_init(128, 10)\n",
        "\n",
        "lr = 0.001\n",
        "BS = 128\n",
        "losses, accuracies = [], []\n",
        "for i in range(1000):\n",
        "  samp = np.random.randint(0, X_train.shape[0], size=(BS))\n",
        "  X = X_train[samp].reshape((-1, 28*28))\n",
        "  Y = Y_train[samp]\n",
        "  x_loss, x_l2, d_l1, d_l2 = forward_backward(X, Y)\n",
        "  \n",
        "  cat = np.argmax(x_l2, axis=1)\n",
        "  accuracy = (cat == Y).mean()\n",
        "  \n",
        "  # SGD\n",
        "  l1 = l1 - lr*d_l1\n",
        "  l2 = l2 - lr*d_l2\n",
        "  \n",
        "  loss = x_loss.mean()\n",
        "  losses.append(loss)\n",
        "  accuracies.append(accuracy)\n",
        "  t.set_description(\"loss %.2f accuracy %.2f\" % (loss, accuracy))\n",
        "  \n",
        "plt.ylim(-0.1, 1.1)\n",
        "plot(losses)\n",
        "plot(accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTFPPiBVqkrX"
      },
      "outputs": [],
      "source": [
        "# evaluate on test set\n",
        "# over 96% with numpy, wow numpy is better than pytorch!\n",
        "# note: add 0.01% more accuracy with float16\n",
        "numpy_eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqQQvZBsqkrY"
      },
      "outputs": [],
      "source": [
        "# for fun, can it recognize 4?\n",
        "m = [[0,0,0,0,0,0,0],\n",
        "     [0,0,1,0,1,0,0],\n",
        "     [0,0,1,0,1,0,0],\n",
        "     [0,0,1,1,1,1,0],\n",
        "     [0,0,0,0,1,0,0],\n",
        "     [0,0,0,0,1,0,0],\n",
        "     [0,0,0,0,0,0,0]]\n",
        "# upscale to 28x28\n",
        "m = np.concatenate([np.concatenate([[x]*4 for x in y]*4) for y in m])\n",
        "imshow(m.reshape(28,28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiKy4JqiqkrZ"
      },
      "outputs": [],
      "source": [
        "# what is it?\n",
        "x = m.reshape(1, -1).dot(l1)\n",
        "x = np.maximum(x, 0)\n",
        "x = x.dot(l2)\n",
        "np.argmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQWEPO6Rqkra"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}